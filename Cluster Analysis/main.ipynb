{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-6bc5f8b6b197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         random_state=0, tol=0.0001, verbose=0)\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inertia'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Initialization complete'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n\u001b[1;32m--> 400\u001b[1;33m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[0;32m    401\u001b[0m     \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\cluster\\_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\cluster\\_k_means.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means._centers_dense\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK - 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import keras\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "import util_mnist_reader\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "x_train, y_train = util_mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_test, y_test = util_mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "m=[]\n",
    "for i in range (7,13):   \n",
    "    kmeans=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "        n_clusters=i, n_init=10, n_jobs=1, precompute_distances='auto',\n",
    "        random_state=0, tol=0.0001, verbose=0)\n",
    "    kmeans.fit(x_train)    \n",
    "    m.append(kmeans.inertia_)\n",
    "    print('inertia',m)\n",
    "plt.figure(1)\n",
    "plt.title('Elbow Graph')\n",
    "plt.xlabel(\"Number of CLusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.plot(range(7,13),m,marker='o', markerfacecolor='blue', markersize=8)\n",
    "o=[]\n",
    "kmeans10=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "        n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
    "        random_state=0, tol=0.0001, verbose=0)\n",
    "kmeans10.fit(x_train)    \n",
    "o.append(kmeans10.inertia_)\n",
    "print('inertia',o)\n",
    "kmeans10.fit(x_train)\n",
    "p3=kmeans10.predict(x_test)\n",
    "k10=kmeans10.inertia_\n",
    "ConfusionMatrix=confusion_matrix(y_test,p3)  #CONFUSION MATRIX\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def _make_cost_m(ConfusionMatrix):\n",
    "    s = np.max(ConfusionMatrix)\n",
    "    return (- ConfusionMatrix + s)\n",
    "ConfusionMatrix = confusion_matrix(y_test, p3)\n",
    "indexes = linear_assignment(_make_cost_m(ConfusionMatrix))\n",
    "js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]\n",
    "cm2 = ConfusionMatrix[:, js]\n",
    "print('ConfusionMatrix')\n",
    "print(cm2)\n",
    "acc=np.trace(cm2) / np.sum(cm2)\n",
    "print('accuracy',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK - 2\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data\n",
    "train_data = extract_data('data/fashion/train-images-idx3-ubyte.gz', 60000)\n",
    "test_data = extract_data('data/fashion/t10k-images-idx3-ubyte.gz', 10000)\n",
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels\n",
    "train_labels = extract_labels('data/fashion/train-labels-idx1-ubyte.gz',60000)\n",
    "test_labels = extract_labels('data/fashion/t10k-labels-idx1-ubyte.gz',10000)\n",
    "# print('xtrainshape',train_data.shape)\n",
    "# print('ytrainshape',test_data.shape)\n",
    "# print('xtest shape',train_labels.shape)\n",
    "# print('ytest',test_labels.shape)\n",
    "label_dict = {\n",
    " 0: 'A',\n",
    " 1: 'B',\n",
    " 2: 'C',\n",
    " 3: 'D',\n",
    " 4: 'E',\n",
    " 5: 'F',\n",
    " 6: 'G',\n",
    " 7: 'H',\n",
    " 8: 'I',\n",
    " 9: 'J',\n",
    "}\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[10], (28,28))\n",
    "curr_lbl = train_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[10], (28,28))\n",
    "curr_lbl = test_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "# Text(0.5,1,'(Label: E)')\n",
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "train_data.shape, test_data.shape\n",
    "# print('xtrainshape',train_data.shape)\n",
    "# print('ytrainshape',test_data.shape)\n",
    "# print('xtest shape',train_labels.shape)\n",
    "# print('ytest',test_labels.shape)\n",
    "\n",
    "train_data.dtype, test_data.dtype\n",
    "np.max(train_data), np.max(test_data)\n",
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)\n",
    "np.max(train_data), np.max(test_data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
    "                                                             train_data,\n",
    "                                                             test_size=0.2,\n",
    "                                                             random_state=13)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "inChannel = 1\n",
    "x, y = 28, 28\n",
    "input_img = Input(shape = (x, y, inChannel))\n",
    "num_classes = 10\n",
    "\n",
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    return conv4\n",
    "\n",
    "def decoder(conv4):    \n",
    "    #decoder\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n",
    "autoencoder.summary()\n",
    "# print('train_x',train_X.shape)\n",
    "# print('valid',valid_X.shape)\n",
    "# print('train_x',train_X.shape)\n",
    "# print('valid',valid_X.shape)\n",
    "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "#kmeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('conv2d_8').output)\n",
    "kmeans = KMeans(n_clusters = 10, verbose=1,init= 'k-means++', n_init=30, random_state=50)\n",
    "encoded = encoder.predict(test_data)\n",
    "# print(encoded.shape)\n",
    "encoder_pred = encoder.predict(test_data).reshape(test_data.shape[0], -1)\n",
    "y_pred = kmeans.fit_predict(encoder_pred)\n",
    "\n",
    "# print(y_pred.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(autoencoder.history.history['loss'], color='#009358', marker='o')\n",
    "plt.plot(autoencoder.history.history['val_loss'], color='red', marker='o')\n",
    "plt.title(\"Training & validation loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "graph = plt.gcf()\n",
    "plt.show()\n",
    "graph.savefig('Task2_3_epoch_loss.png', dpi=300)\n",
    "\n",
    "\n",
    "# print(test_labels.shape)\n",
    "# print(y_pred.shape)\n",
    "cm = metrics.confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "k_labels = y_pred  # Get cluster labels\n",
    "k_labels_matched = np.empty_like(k_labels)\n",
    "\n",
    "# For each cluster label...\n",
    "for k in np.unique(k_labels):\n",
    "\n",
    "    # ...find and assign the best-matching truth label\n",
    "    match_nums = [np.sum((k_labels==k)*(test_labels==t)) for t in np.unique(test_labels)]\n",
    "    k_labels_matched[k_labels==k] = np.unique(test_labels)[np.argmax(match_nums)]\n",
    "\n",
    "cm1 = metrics.confusion_matrix(test_labels, k_labels_matched)\n",
    "\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def _make_cost_m(cm):\n",
    "    s = np.max(cm)\n",
    "    return (- cm + s)\n",
    "\n",
    "indexes = linear_assignment(_make_cost_m(cm))\n",
    "js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]\n",
    "cm2 = cm[:, js]\n",
    "\n",
    "print (\"Task 2 Confusion Matrix\")\n",
    "print(cm2)\n",
    "\n",
    "print(\"(Task2) Accuracy of K-Means:\",np.trace(cm2) / np.sum(cm2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK - 3\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data\n",
    "train_data = extract_data('data/fashion/train-images-idx3-ubyte.gz', 60000)\n",
    "test_data = extract_data('data/fashion/t10k-images-idx3-ubyte.gz', 10000)\n",
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels\n",
    "train_labels = extract_labels('data/fashion/train-labels-idx1-ubyte.gz',60000)\n",
    "test_labels = extract_labels('data/fashion/t10k-labels-idx1-ubyte.gz',10000)\n",
    "\n",
    "# print('xtrainshape',train_data.shape)\n",
    "# print('ytrainshape',test_data.shape)\n",
    "# print('xtest shape',train_labels.shape)\n",
    "# print('ytest',test_labels.shape)\n",
    "label_dict = {\n",
    " 0: 'A',\n",
    " 1: 'B',\n",
    " 2: 'C',\n",
    " 3: 'D',\n",
    " 4: 'E',\n",
    " 5: 'F',\n",
    " 6: 'G',\n",
    " 7: 'H',\n",
    " 8: 'I',\n",
    " 9: 'J',\n",
    "}\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[10], (28,28))\n",
    "curr_lbl = train_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[10], (28,28))\n",
    "curr_lbl = test_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "# Text(0.5,1,'(Label: E)')\n",
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "train_data.shape, test_data.shape\n",
    "# print('xtrainshape',train_data.shape)\n",
    "# print('ytrainshape',test_data.shape)\n",
    "# print('xtest shape',train_labels.shape)\n",
    "# print('ytest',test_labels.shape)\n",
    "\n",
    "train_data.dtype, test_data.dtype\n",
    "np.max(train_data), np.max(test_data)\n",
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)\n",
    "np.max(train_data), np.max(test_data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
    "                                                             train_data,\n",
    "                                                             test_size=0.2,\n",
    "                                                             random_state=13)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "inChannel = 1\n",
    "x, y = 28, 28\n",
    "input_img = Input(shape = (x, y, inChannel))\n",
    "num_classes = 10\n",
    "\n",
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    return conv4\n",
    "\n",
    "def decoder(conv4):    \n",
    "    #decoder\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n",
    "autoencoder.summary()\n",
    "# print('train_x',train_X.shape)\n",
    "# print('valid',valid_X.shape)\n",
    "# print('train_x',train_X.shape)\n",
    "# print('valid',valid_X.shape)\n",
    "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "epochs = range(100)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss', color='blue')\n",
    "plt.plot(epochs, val_loss, 'b.-', label='Validation loss',color='green')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "autoencoder.save_weights('autoencoder.h5')\n",
    "\n",
    "#gmm\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('conv2d_8').output)\n",
    "encoded = encoder.predict(test_data)\n",
    "encoder_pred = encoder.predict(test_data).reshape(test_data.shape[0], -1)\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, covariance_type='spherical', tol=0.02, verbose=1, n_init=10, init_params='kmeans', random_state=35 )\n",
    "\n",
    "y_pred_gmm = gmm.fit_predict(encoder_pred)\n",
    "\n",
    "# print(y_pred_gmm.shape)\n",
    "\n",
    "cm = metrics.confusion_matrix(test_labels, y_pred_gmm)\n",
    "\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def _make_cost_m(cm):\n",
    "    s = np.max(cm)\n",
    "    return (- cm + s)\n",
    "\n",
    "indexes = linear_assignment(_make_cost_m(cm))\n",
    "js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]\n",
    "cm2 = cm[:, js]\n",
    "\n",
    "print (\"Task 3 Confusion Matrix\")\n",
    "print(cm2)\n",
    "\n",
    "print(\"(Task3) Accuracy of GMM:\",np.trace(cm2) / np.sum(cm2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
